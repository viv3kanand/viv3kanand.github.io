
# Problem is Drawing from Distribution

* MCMC is used to solve the problem of sampling from a complication distribtution
* We have a distribution, D, over baby names such that given any baby name we get the probability of that baby's name.

> Problem is we want to efficiently draw a name from D, but we have no idea what process was used to make D.

## The sampling problem

* Let D be a distribution over a finite set X. You are given black-box access to the probability distribution function p(x) which outputs the probability of drawing x \in X according to D. Design an efficient randomized algorithm A which outputs an element of X so that the probability of outputting x is approximately p(x). More generally, output a sample of elements from X drawn according to p(x).

* Markov Chain is essentially a fancy term for a random walk on a graph.
* The main theorem we need to do anything useful with Markov chains is the stationary distribution theorem (sometimes called the “Fundamental Theorem of Markov Chains,” and for good reason). What it says intuitively is that for a very long random walk, the probability that you end at some vertex v is independent of where you started! 





# Create Test Data

Here we create some linear relationship between x and y. But we add some noise in the y-value.

```{r}
trueA <- 5
trueB <- 0 # y-intercept
trueSd <- 10
sampleSize <- 31
 
# create independent x-values 
x <- (-(sampleSize-1)/2):((sampleSize-1)/2)

# create dependent values according to ax + b + N(0,sd)
y <-  trueA * x + trueB + rnorm(n = sampleSize, mean = 0, sd = trueSd)
 
plot(x,y, main="Test Data")
```

# Defining the statistical model
***

In machine learning, there are 3 elements to any model:

1. Representation (Hypothesis Function)
2. Evaluation (Loss Function; Objective Function)
3. Optimization

The representation of our statistical model will be:

$$Y = aX + b + N(0, sd)$$

# Defining the Likelihood (Loss Function)
***

We define our likelihood function (aka. loss/objective function). In brief, a likelihood function is:

> probability (density) with which we would expect the observed data to occur conditional on the parameters of the model that we look at. 

The loss function of a traditional simple linear regression is:

$$L = \sum_{i=1}^{n}(Y_{i} - (\beta_{0} + \beta_{1}X_{i}))^2$$

```{r}
#' @param param Vector of length x containing slope, y-intercept, and standard
#'   deviation.
likelihood <- function(param) {

	a <- param[1]
	b <- param[2]

	# param[3] = trueSd
	in.sd <- param[3]
		
	# Make predictions given slope (a)
	# NB: x is fixed data
	# NB: b == trueB
	pred <- a*x + b

	# log is used to get log likelihoods
	# For each pred, we run dnorm with the pred as a mean, in.sd (fixed across
	# dnorm runs) with the corresponding y value. 
	#
	# The idea is to get the probability density of observing the deviation of
	# prediction to the true value. We then sum up the probability density values
	# across all of these predictions.
	singlelikelihoods <- dnorm(y, mean = pred, sd = in.sd, log = TRUE)
	sumll <- sum(singlelikelihoods)
	sumll
}

# Example: plot the likelihood profile of the slope a

#' @param x Slope of the line
slopevalues <- function(in.slope) {
	# trueB is fixed
	# trueSd is fixed
	likelihood(param = c(in.slope, trueB, trueSd))
}

# Produce a vector of slopes
slope <- seq(3, 7, by = .05)
in.slope <- 3

# Run slopevalues for each slope value
slopelikelihoods <- lapply(slope, slopevalues)

plot(seq(3, 7, by=.05), slopelikelihoods , type = "l", 
		 xlab = "values of slope parameter a", 
		 ylab = "Log likelihood")
```

# Defining the Prior
***

We need to specify a prior distribution for each parameter. Here we use uniform distributions and normal distributions for all three parameters.

```{r prior}
# Prior distribution
prior <- function(param){
	a = param[1]
	b = param[2]
	sd = param[3]

	# Uniform distribution
	aprior = dunif(a, min=0, max=10, log = T)

	# Normal distribution
	bprior = dnorm(b, sd = 5, log = T)

	# Uniform distribution
	sdprior = dunif(sd, min=0, max=30, log = T)

	aprior + bprior + sdprior
}
```

# The posterior
***

The product of prior and likelihood is the actual quantity the MCMC will be working on.

```{r}
posterior <- function(param){
   return (likelihood(param) + prior(param))
}
```

# The MCMC
***

```{r}
######## Metropolis algorithm ################
 
# Proposal Function
# Choosing a new parameter value close to the old value based on some 
# probability density that is called the proposal function
proposalfunction <- function(param = c(4, 0, 10)){

	# Generate random values from normal distribution
	rnorm(3, mean = param, sd = c(0.1, 0.5, 0.3))

}
 
# Run Metropolis MCMC Algorithm
run_metropolis_MCMC <- function(startvalue = c(4, 0, 10), iterations = 10000){

	# determine iterations + 1 (nrow) by 3 (ncol) matrix 
	chain <- array(dim = c(iterations+1, 3))

	# Set startvalue	
	chain[1,] <- startvalue

	for (i in 1:iterations){

		# Generate proposal function?
		proposal <- proposalfunction(chain[i,])
			
		probab = exp(posterior(proposal) - posterior(chain[i,]))

		if (runif(1) < probab){
				chain[i+1,] = proposal
		}else{
				chain[i+1,] = chain[i,]
		}
	}
	return(chain)
}
 
startvalue <- c(4, 0, 10)
chain <- run_metropolis_MCMC(startvalue, 10000)
 
burnIn = 5000
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
```
>

# References
***

* [Markov Chain Monte Carlo Without all the BS](http://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/)
