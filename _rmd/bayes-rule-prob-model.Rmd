---
title: "Bayes' Rule"
date: "March 17th, 2016"
layout: post
output:
  html_document
tags: [R, stats]
---

In my previous post on [Joint, Marginal, and Conditional Probabilities post]({% post_url 2016-03-20-basic-prob %}), I introduced the 3 different types of probabilities. One famous probability rule that is built on these probabilities is called "Bayes' Rule" which forms the basis of bayesian statistics. 

Here is an overview of what will be discussed in this post.

**Table of Contents**

<ul data-toc="body" data-toc-headings="h2,h3"></ul>

```{r echo = FALSE}
library("knitr")
library("captioner")

tbl.nums <- captioner(prefix = "<u>Table</u>", css_class = "tblcaption")
```

Bayes' rules can be derived by starting with the conditional probability of <span class="inlinecode">$P(X\ |\ Y)$</span> which is represented as:

<div>
$$P(X\ |\ Y) = \frac{P(X, Y)}{P(Y)}$$
</div>

By multiplying this with P(Y(), we get:

<div>
$$P(X\ |\ Y)\ P(Y) = P(X, Y)$$
</div>

We can also perform this same algebraic manipulation for <span class="inlinecode">$P(Y\ |\ X)$</span>:

<div>
$$\begin{align}
P(Y\ |\ X) &= \frac{P(X, Y)}{P(X)} \\
P(Y\ |\ X)\ P(X) &= P(X, Y)
\end{align}$$
</div>

Notice how we now have two alternative representations of the joint probability <span class="inlinecode">$P(X, Y)$</span> which we can equate to each other:

<div>
$$\begin{align}
P(X\ |\ Y)\ P(Y) &= P(X, Y) \\
P(Y\ |\ X)\ P(X) &= P(X, Y) \\
P(X\ |\ Y)\ P(Y) &= P(Y\ |\ X)\ P(X) 
\end{align}$$
</div>

If we now divide by <span class="inlinecode">$P(Y)$</span>:

<div>
$$P(X\ |\ Y) = \frac{P(Y\ |\ X)\ P(X)}{P(Y)},\ if\ P(Y) \neq 0$$
</div>

This is defined as the Bayes' rule. Essentially what we have done is relate two different, but related conditional probability equations to each other. 

## From Prior Belief to Updated Belief

At first glance, Bayes' rule seems pretty simple and might not be obvious as to why it is so useful. I mean all we've done is just rewritten the conditional probability equation right? While that is true, the power comes from how you intepret the rule and define the variables. 

Let's say we have a hypothesis (H) and some data (E) to support or oppose this hypothesis. If we make our X (from above) equal to H and Y equal to E, then our Bayes' rule becomes:

<div>
$$P(H\ |\ E) = \frac{P(E\ |\ H)\ P(H)}{P(E)}$$
</div>

So what does this mean? Well <span class="inlinecode">$P(H)$</span> represents our "prior" belief of what the hypothesis is before we see any data. This prior is informed by our "expert" opinion on the hypothesis and thus is subjective. But the beauty of this rule, is that it allows us to relate this prior, <span class="inlinecode">$P(H)$</span>, to an "updated belief" of the hypothesis <span class="inlinecode">$P(H\ |\ E)$</span> once we have some data to consider. In this bayesian terms, this "updated belief" is called the posterior.

Let's see this rule in action now.

### Example of Bayes' Rule Using Playing Cards

<div class="alert alert-dismissible alert-info">
<h4>Heads Up!</h4>
This example is shamelessly borrowed from [Bayes' Theorem and Conditional Probability](https://brilliant.org/wiki/bayes-theorem/)
</div>

Say we are playing cards and we have a prior probability that there is a <span class="inlinecode">$\frac{1}{13}$</span> chance of getting a King card. How do we know this? Well our expert knowledge tells us that there are 52 cards in a deck with a total of 4 suits (hearts, diamonds, clubs, and jacks). Each suit has its own Jack, Queen, King (i.e. face card), and so there must be 4 Kings in total. Therefore, <span class="inlinecode">$\frac{4}{52} = \frac{1}{13}$</span>. 

But now say, we also have the data that the card we have in hand is a face card. What then is the probability of getting a King now? What we really are asking is what is our updating belief (posterior), <span class="inlinecode">$P(H\ |\ E)$</span>, if getting a King in light of this new information. We can use Baye's Rule to help us solve this. Let's break down the individual components of Bayes' rule here:

* P(H) = <span class="inlinecode">$\frac{1}{13}$</span>.
* P(E) = As there are 3 face cards per suit and there are 4 suits, then this is <span class="inlinecode">$\frac{12}{52} = \frac{3}{13}$</span>
* P(E | H) = Since every single King card is a face card, this probability has to be 1.

Now we just fill this in into the Bayes' Equation:

<div>
$$\begin{align}
P(H\ |\ E) &= \frac{1 * \frac{1}{13}}{\frac{3}{13}} \\
P(H\ |\ E) &= \frac{1}{13}
\end{align}$$
</div>

Basically we started with a prior belief, <span class="inlinecode">$P(H) = \frac{1}{13}$</span>, but with new information regarding the card being a face card we are more confident that the card is a King now, <span class="inlinecode">$P(H\ |\ E) = \frac{1}{13}$</span>

## Disease Testing Example

This particular example uses the joint probabilities, but specifically the bayes rule expresses the joint probabilities as <span class="inlinecode">$P(Y\ |\ X)\ P(X)$</span>. We will use another example that utilizies this expression.

* <span class="inlinecode">$D$</span>: Represents the true presence (1) and absence (0) of a disease.
* <span class="inlinecode">$P(D) = 0.001$</span>: Prior belief that a person selected at random has the disease.
* 99% hit rate for the test. This means that if a person has the disease, then the test result is positive 99% of the time.
    + T = +: Positive test result
    + T = -: Negative test result
* Observed test result is the datum that we use to modify out belief about the value of the underlying disease parameter.
* Hit rate is expressed as <span class="inlinecode">$P(T = 1\ |\ D = 1) = 0.99$</span>
* Test has false positive rate of 5%: <span class="inlinecode">$P(T = 1\ |\ D = 0) = 0.05$</span>

We can summarize all this information into a two-way table:

| Test result (T)    | <span class="inlinecode">$D = 1$</span>                             | <span class="inlinecode">$D = 0$</span>                             | 
|:------------------:|:------------------------------------------------------------------------:|:------------------------------------------------------------------------:|
| 1                  | <span class="inlinecode">$P(T = 1\ |\ D = 1)\ P(D = 1)$</span> = 0.99 | <span class="inlinecode">$P(T = 1\ |\ D = 0)\ P(D = 0)$</span> |
| 0                  | <span class="inlinecode">$P(T = 0\ |\ D = 0)\ P(D = 1)$</span> | <span class="inlinecode">$P(T = 0\ |\ D = 0)\ P(D = 0)$</span> |
| Marginal (disease) | <span class="inlinecode">$P(D = 1)$</span>                          | <span class="inlinecode">$P(D = 0)$</span>                          |

Say we sampled a person from the population, gave them the test, and the test came back positive. What is the posterior probability the person has the disease? Our inituition might be that the probability that this person has the disease is pretty high because we know that the hit rate of the test is 99%. But when we apply bayes rule:

<div>
$$P(D = 1\ |\ T = 1) = \frac{P(T = 1\ |\ D = 1)\ P(D = 1)}{P(T = 1)}$$
</div>

## Bayes' Rule for Inferring Model Parameters

Bayes rule becomes key when we start talking about probabilistic model. Specifically in a probabilistic framework, each data point (D) has an assigned probability given some model structure and parameters:

<div>
$$P(D\ |\ \theta)$$
</div>

By using Bayes' rules, we can calculate the inverse conditional probability:

<div>
$$P(\theta\ |\ D)$$
</div>


inferring model parameters. If we go back to our two-way table, and we 

<table class="table table-striped table-hover">
<thead>
<tr>
<th rowspan="2">Data</th>
<th colspan="4">Model Parameter</th>
</tr>
<tr>
<th><span class="inlinecode">$_{1}$</span></th>
<th>.</th>
<th><span class="inlinecode">$\theta$</span></th>
<th>.</th>
</tr>
</thead>
  <tr>
  <td><span class="inlinecode">$X_{1}$</span></td>
  </tr>
<tr>
<td>.</td>
</tr>
<tr>
  <td><span class="inlinecode">$X_{i}$</span></td>
</tr>
</table>


## Revisiting the Bayes' Rule

* Bayes rule is typically used to infer model parameters from data

The different parts of the rule have specific:

* <span class="inlinecode">$P(X)$</span>: Prior probability. This is our prior belief of X before we factor in any other variables. In actualty, it's just a marginal probability but with a fancy name.
* <span class="inlinecode">$P(X\ |\ Y )$</span>: Posterior probability. This is the conditional probability of X given Y.
* <span class="inlinecode">$P(Y\ |\ X )$</span>: Likelihood. This is the conditional probability of Y given X. 
* <span class="inlinecode">$P(Y)$</span>: Marginal probability. This is just the probability of Y before we factor in any other variables. 

Don't be intimated by these names. In actuality, these are just fancy "bayesian terms" for probabilities we have already discussed in a previous post. **There is nothing mathematically different between a "prior probability" and a "marginal probability"**. It's just termed that way because of the context when doing bayesian statistics/inferences.

## References

* [Understanding Bayes](http://alexanderetz.com/understanding-bayes/)
* [Count Bayesie - Bayes' Theorem with Lego](https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego)
* [Bayesian machine learning](http://fastml.com/bayesian-machine-learning/)
* [Where priors come from](http://zinkov.com/posts/2015-06-09-where-priors-come-from/)
* [What is the difference between Bayes Theorem and conditional probability and how do I know when to apply them?](https://www.quora.com/What-is-the-difference-between-Bayes-Theorem-and-conditional-probability-and-how-do-I-know-when-to-apply-them)
* [Bayes' Theorem and Conditional Probability](https://brilliant.org/wiki/bayes-theorem/)

## R Session

```{r session, echo = FALSE}
devtools::session_info()
```
